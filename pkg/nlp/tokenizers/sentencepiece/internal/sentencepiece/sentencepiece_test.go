// Copyright (c) 2020 Vikesh Raj C. All rights reserved.
// This work is licensed under the terms of the MIT license.
// For a copy, see <https://opensource.org/licenses/MIT>.

package sentencepiece

import (
	"reflect"
	"testing"
)

func TestTokenization(t *testing.T) {
	sp, err := NewSentencepieceFromFile("test_data/xlnet-base-cased-spiece.model", false)
	if err != nil {
		t.Errorf("Unable to create sentencepiece")
		return
	}

	tests := []struct {
		text   string
		tokens []Token
	}{
		{text: "this", tokens: []Token{{ID: 52, Text: "â–this"}}},
		{text: "hello", tokens: []Token{{ID: 24717, Text: "â–hello"}}},
		{text: "This is a sample sentence to be tokeÌnized", tokens: []Token{
			{ID: 122, Text: "â–This"},
			{ID: 27, Text: "â–is"},
			{ID: 24, Text: "â–a"},
			{ID: 4561, Text: "â–sample"},
			{ID: 3833, Text: "â–sentence"},
			{ID: 22, Text: "â–to"},
			{ID: 39, Text: "â–be"},
			{ID: 22, Text: "â–to"},
			{ID: 267, Text: "k"},
			{ID: 0, Text: "Ã©"},
			{ID: 180, Text: "n"},
			{ID: 1227, Text: "ized"},
		}},
		{text: "Wondering how this will get tokenized ğŸ¤” ?", tokens: []Token{
			{ID: 14748, Text: "â–Wonder"},
			{ID: 56, Text: "ing"},
			{ID: 160, Text: "â–how"},
			{ID: 52, Text: "â–this"},
			{ID: 53, Text: "â–will"},
			{ID: 133, Text: "â–get"},
			{ID: 17366, Text: "â–token"},
			{ID: 1227, Text: "ized"},
			{ID: 17, Text: "â–"},
			{ID: 0, Text: "ğŸ¤”"},
			{ID: 17, Text: "â–"},
			{ID: 82, Text: "?"},
		}},
		{text: "Ä°s th!s ğ©¸½ Ïº Å Å“ UgljÅ¡iÄ‡ dáº¥u náº·ng", tokens: []Token{
			{ID: 17, Text: "â–"},
			{ID: 0, Text: "Ä°"},
			{ID: 23, Text: "s"},
			{ID: 17, Text: "â–"},
			{ID: 138, Text: "th"},
			{ID: 136, Text: "!"},
			{ID: 23, Text: "s"},
			{ID: 17, Text: "â–"},
			{ID: 0, Text: "ğ©¸½"},
			{ID: 17, Text: "â–"},
			{ID: 0, Text: "Ïº"},
			{ID: 17, Text: "â–"},
			{ID: 0, Text: "Å "},
			{ID: 128, Text: "â–U"},
			{ID: 15222, Text: "gl"},
			{ID: 1315, Text: "j"},
			{ID: 0, Text: "Å¡"},
			{ID: 150, Text: "i"},
			{ID: 0, Text: "Ä‡"},
			{ID: 17, Text: "â–"},
			{ID: 66, Text: "d"},
			{ID: 0, Text: "áº¥"},
			{ID: 660, Text: "u"},
			{ID: 17, Text: "â–"},
			{ID: 180, Text: "n"},
			{ID: 0, Text: "áº·"},
			{ID: 3511, Text: "ng"},
		}},
		{text: "compose email to john saying i will be running late to office today because i am not feeling well, my head is aching and in the body add shall we meet next week and when we go to the office lets reach by around 10 am and go for a movie in the evening, may be Spiderman which seems to be a very good movie which got 5 star review from rottentomatoes and imdb", tokens: []Token{
			{ID: 23391, Text: "â–compose"},
			{ID: 1706, Text: "â–email"},
			{ID: 22, Text: "â–to"},
			{ID: 17, Text: "â–"},
			{ID: 22116, Text: "john"},
			{ID: 591, Text: "â–saying"},
			{ID: 17, Text: "â–"},
			{ID: 150, Text: "i"},
			{ID: 53, Text: "â–will"},
			{ID: 39, Text: "â–be"},
			{ID: 926, Text: "â–running"},
			{ID: 471, Text: "â–late"},
			{ID: 22, Text: "â–to"},
			{ID: 495, Text: "â–office"},
			{ID: 494, Text: "â–today"},
			{ID: 149, Text: "â–because"},
			{ID: 17, Text: "â–"},
			{ID: 150, Text: "i"},
			{ID: 569, Text: "â–am"},
			{ID: 50, Text: "â–not"},
			{ID: 1803, Text: "â–feeling"},
			{ID: 143, Text: "â–well"},
			{ID: 19, Text: ","},
			{ID: 94, Text: "â–my"},
			{ID: 291, Text: "â–head"},
			{ID: 27, Text: "â–is"},
			{ID: 24, Text: "â–a"},
			{ID: 5410, Text: "ching"},
			{ID: 21, Text: "â–and"},
			{ID: 25, Text: "â–in"},
			{ID: 18, Text: "â–the"},
			{ID: 458, Text: "â–body"},
			{ID: 1319, Text: "â–add"},
			{ID: 1530, Text: "â–shall"},
			{ID: 80, Text: "â–we"},
			{ID: 767, Text: "â–meet"},
			{ID: 244, Text: "â–next"},
			{ID: 260, Text: "â–week"},
			{ID: 21, Text: "â–and"},
			{ID: 90, Text: "â–when"},
			{ID: 80, Text: "â–we"},
			{ID: 216, Text: "â–go"},
			{ID: 22, Text: "â–to"},
			{ID: 18, Text: "â–the"},
			{ID: 495, Text: "â–office"},
			{ID: 10234, Text: "â–lets"},
			{ID: 1287, Text: "â–reach"},
			{ID: 37, Text: "â–by"},
			{ID: 199, Text: "â–around"},
			{ID: 241, Text: "â–10"},
			{ID: 569, Text: "â–am"},
			{ID: 21, Text: "â–and"},
			{ID: 216, Text: "â–go"},
			{ID: 28, Text: "â–for"},
			{ID: 24, Text: "â–a"},
			{ID: 1432, Text: "â–movie"},
			{ID: 25, Text: "â–in"},
			{ID: 18, Text: "â–the"},
			{ID: 2060, Text: "â–evening"},
			{ID: 19, Text: ","},
			{ID: 132, Text: "â–may"},
			{ID: 39, Text: "â–be"},
			{ID: 17489, Text: "â–Spider"},
			{ID: 249, Text: "man"},
			{ID: 59, Text: "â–which"},
			{ID: 1303, Text: "â–seems"},
			{ID: 22, Text: "â–to"},
			{ID: 39, Text: "â–be"},
			{ID: 24, Text: "â–a"},
			{ID: 172, Text: "â–very"},
			{ID: 195, Text: "â–good"},
			{ID: 1432, Text: "â–movie"},
			{ID: 59, Text: "â–which"},
			{ID: 345, Text: "â–got"},
			{ID: 306, Text: "â–5"},
			{ID: 1795, Text: "â–star"},
			{ID: 1398, Text: "â–review"},
			{ID: 40, Text: "â–from"},
			{ID: 28626, Text: "â–rotten"},
			{ID: 261, Text: "to"},
			{ID: 18693, Text: "mato"},
			{ID: 202, Text: "es"},
			{ID: 21, Text: "â–and"},
			{ID: 7693, Text: "â–im"},
			{ID: 66, Text: "d"},
			{ID: 508, Text: "b"},
		}},
	}

	for _, test := range tests {
		output := sp.Tokenize(test.text)
		if !reflect.DeepEqual(output, test.tokens) {
			t.Errorf("Tokenization error : %s, len %d, got %v || expected %v", test.text, len(test.text), output, test.tokens)
		}
	}
}

func TestTokenizationSPM(t *testing.T) {
	sp, err := NewSentencepieceFromFile("test_data/spm.model", true)
	if err != nil {
		t.Errorf("Unable to create sentencepiece")
		return
	}

	tests := []struct {
		text   string
		tokens []Token
	}{
		{text: "this", tokens: []Token{{ID: 48, Text: "â–this"}}},
		{text: "hello", tokens: []Token{{ID: 10975, Text: "â–hello"}}},
		{text: "This is a sample sentence to be tokeÌnized", tokens: []Token{
			{ID: 48, Text: "â–this"},
			{ID: 25, Text: "â–is"},
			{ID: 21, Text: "â–a"},
			{ID: 5717, Text: "â–sample"},
			{ID: 5123, Text: "â–sentence"},
			{ID: 20, Text: "â–to"},
			{ID: 44, Text: "â–be"},
			{ID: 20, Text: "â–to"},
			{ID: 197, Text: "k"},
			{ID: 1, Text: "Ã©"},
			{ID: 103, Text: "n"},
			{ID: 1333, Text: "ized"},
		}},
		{text: ".", tokens: []Token{{ID: 13, Text: "â–"}, {ID: 9, Text: "."}}},
		{text: "this is a dot .", tokens: []Token{
			{ID: 48, Text: "â–this"},
			{ID: 25, Text: "â–is"},
			{ID: 21, Text: "â–a"},
			{ID: 14123, Text: "â–dot"},
			{ID: 13, Text: "â–"},
			{ID: 9, Text: "."},
		}},
		{text: "compose email to john saying i will be running late to office today because i am not feeling well, my head is aching and in the body add shall we meet next week and when we go to the office lets reach by around 10 am and go for a movie in the evening, may be Spiderman which seems to be a very good movie which got 5 star review from rottentomatoes and imdb", tokens: []Token{
			{ID: 18217, Text: "â–compose"},
			{ID: 8517, Text: "â–email"},
			{ID: 20, Text: "â–to"},
			{ID: 239, Text: "â–john"},
			{ID: 1148, Text: "â–saying"},
			{ID: 31, Text: "â–i"},
			{ID: 129, Text: "â–will"},
			{ID: 44, Text: "â–be"},
			{ID: 946, Text: "â–running"},
			{ID: 456, Text: "â–late"},
			{ID: 20, Text: "â–to"},
			{ID: 488, Text: "â–office"},
			{ID: 786, Text: "â–today"},
			{ID: 185, Text: "â–because"},
			{ID: 31, Text: "â–i"},
			{ID: 589, Text: "â–am"},
			{ID: 52, Text: "â–not"},
			{ID: 1249, Text: "â–feeling"},
			{ID: 134, Text: "â–well"},
			{ID: 15, Text: ","},
			{ID: 51, Text: "â–my"},
			{ID: 157, Text: "â–head"},
			{ID: 25, Text: "â–is"},
			{ID: 17010, Text: "â–aching"},
			{ID: 17, Text: "â–and"},
			{ID: 19, Text: "â–in"},
			{ID: 14, Text: "â–the"},
			{ID: 358, Text: "â–body"},
			{ID: 3547, Text: "â–add"},
			{ID: 3004, Text: "â–shall"},
			{ID: 95, Text: "â–we"},
			{ID: 1255, Text: "â–meet"},
			{ID: 328, Text: "â–next"},
			{ID: 877, Text: "â–week"},
			{ID: 17, Text: "â–and"},
			{ID: 76, Text: "â–when"},
			{ID: 95, Text: "â–we"},
			{ID: 162, Text: "â–go"},
			{ID: 20, Text: "â–to"},
			{ID: 14, Text: "â–the"},
			{ID: 488, Text: "â–office"},
			{ID: 6884, Text: "â–lets"},
			{ID: 1470, Text: "â–reach"},
			{ID: 34, Text: "â–by"},
			{ID: 140, Text: "â–around"},
			{ID: 332, Text: "â–10"},
			{ID: 589, Text: "â–am"},
			{ID: 17, Text: "â–and"},
			{ID: 162, Text: "â–go"},
			{ID: 26, Text: "â–for"},
			{ID: 21, Text: "â–a"},
			{ID: 1308, Text: "â–movie"},
			{ID: 19, Text: "â–in"},
			{ID: 14, Text: "â–the"},
			{ID: 2089, Text: "â–evening"},
			{ID: 15, Text: ","},
			{ID: 123, Text: "â–may"},
			{ID: 44, Text: "â–be"},
			{ID: 5650, Text: "â–spider"},
			{ID: 177, Text: "man"},
			{ID: 56, Text: "â–which"},
			{ID: 2206, Text: "â–seems"},
			{ID: 20, Text: "â–to"},
			{ID: 44, Text: "â–be"},
			{ID: 21, Text: "â–a"},
			{ID: 253, Text: "â–very"},
			{ID: 254, Text: "â–good"},
			{ID: 1308, Text: "â–movie"},
			{ID: 56, Text: "â–which"},
			{ID: 330, Text: "â–got"},
			{ID: 331, Text: "â–5"},
			{ID: 778, Text: "â–star"},
			{ID: 1487, Text: "â–review"},
			{ID: 37, Text: "â–from"},
			{ID: 11573, Text: "â–rotten"},
			{ID: 262, Text: "to"},
			{ID: 8844, Text: "mato"},
			{ID: 160, Text: "es"},
			{ID: 17, Text: "â–and"},
			{ID: 797, Text: "â–im"},
			{ID: 9007, Text: "db"},
		}},
	}

	for _, test := range tests {
		output := sp.Tokenize(test.text)
		if !reflect.DeepEqual(output, test.tokens) {
			t.Errorf("Tokenization error : %s, len %d, got %v || expected %v", test.text, len(test.text), output, test.tokens)
		}
	}
}

func TestControlWords(t *testing.T) {
	sp, err := NewSentencepieceFromFile("test_data/xlnet-base-cased-spiece.model", false)
	if err != nil {
		t.Errorf("Unable to create sentencepiece")
		return
	}

	unknownIndex := sp.GetUnknownIndex()
	if unknownIndex != 0 {
		t.Errorf("Unknown index not equal to 0")
	}

	clsIndex, ok := sp.GetControlWord("<cls>")
	if !ok || clsIndex != 3 {
		t.Errorf("Control word [CLS] not correct : %d", clsIndex)
	}

}

func TestControlWords2(t *testing.T) {
	sp, err := NewSentencepieceFromFile("test_data/spm.model", true)
	if err != nil {
		t.Errorf("Unable to create sentencepiece")
		return
	}

	unknownIndex := sp.GetUnknownIndex()
	if unknownIndex != 1 {
		t.Errorf("Unknown index not equal to 1")
	}

	clsIndex, ok := sp.GetControlWord("[CLS]")
	if !ok || clsIndex != 2 {
		t.Errorf("Control word [CLS] not correct")
	}
}

func BenchmarkSentencePiece(b *testing.B) {
	sp, err := NewSentencepieceFromFile("test_data/xlnet-base-cased-spiece.model", false)
	if err != nil {
		b.Errorf("Unable to create sentencepiece")
		return
	}

	b.ResetTimer()

	inputs := []string{
		"compose email to john saying i will be running late to office today because i am not feeling well, my head is aching and in the body add shall we meet next week and when we go to the office lets reach by around 10 am and go for a movie in the evening, may be Spiderman which seems to be a very good movie which got 5 star review from rottentomatoes and imdb",
	}

	for _, input := range inputs {
		b.Run(firstNChars(input, 20), func(b *testing.B) {
			for i := 0; i < b.N; i++ {
				sp.Tokenize(input)
			}
		})
	}
}

func firstNChars(s string, n int) string {
	if len(s) < n {
		return s
	}
	return s[:n]
}
